---

# File: roles/base/tasks/main.yml

- name: Define manage_dnsmasq
  set_fact:
    manage_dnsmasq: false
  when: manage_dnsmasq is not defined
  tags: [ always ]

- name: Define manage_resolv_conf
  set_fact:
    manage_resolv_conf: false
  when: manage_resolv_conf is not defined
  tags: [ always ]

- name: Define manage_interfaces
  set_fact:
    manage_interfaces: false
  when: manage_interfaces is not defined
  tags: [ always ]

- fail: msg="Variable 'packages_install' is not defined.
    Make sure inventory/all.yml includes '{{ inventory_hostname }}' in group '{{ ansible_distribution|lowercase }}'"
  when: packages_install is not defined
  tags: [ base, packages, updates ]

  # This is not DRY, but base role does not use pre_tasks.yml
- name: Check to see if dims.logger exists yet
  stat:
    path: '{{ dims_bin }}/dims.logger'
  register: _logger_stat
  tags: [ base, config, dns, logrotate, packages, rsyslogd, tests, triggers ]

- name: Log start of 'base' role
  shell: >
    {{ dims_bin }}/dims.logger
    --logger-tag ansible-playbook
    --logmon-exchange {{ dims_logmon_exchange|default('devops') }}
    'Starting role: base'
  when: _logger_stat is defined and _logger_stat.stat.exists
  ignore_errors: yes
  tags: [ base, config, dns, logrotate, packages, rsyslogd, tests, triggers ]

- name: Check to see if update-manager is running on Ubuntu
  shell: "ps aux | awk '/[ ]\\/usr\\/bin\\/update-manager/ { print $2; }'"
  register: _ps
  ignore_errors: yes
  when: ansible_distribution == "Ubuntu"
  tags: [ base, packages, updates ]

- name: Kill update_manager to avoid dpkg lock contention
  shell: "kill {{ _ps.stdout }}"
  become: yes
  when: ansible_distribution == "Ubuntu"
        and _ps.stdout is defined and _ps.stdout|int != 0
  tags: [ base, packages, updates ]

- name: Check to see if gpk-update-viewer is running on Ubuntu
  shell: "ps aux | awk '/[ ]gpk-update-viewer/ { print $2; }'"
  register: _ps
  ignore_errors: yes
  when: ansible_distribution == "Debian"
  tags: [ base, packages, updates ]

- name: Kill gpk-update-viewer to avoid dpkg lock contention
  shell: "kill {{ _ps.stdout }}"
  become: yes
  when: ansible_distribution == "Debian"
        and _ps.stdout is defined and _ps.stdout|int != 0
  tags: [ base, packages, updates ]

- name: Make sure blacklisted packages are absent (Debian)
  apt: state=absent name={{ item }} force=yes
  with_items:
    - '{{ packages_remove }}'
  become: yes
  when: ansible_pkg_mgr == "apt"
  ignore_errors: yes
  tags: [ base, packages, updates ]

- name: Only "update_cache=yes" if >3600s since last update (Debian)
  apt: update_cache=yes cache_valid_time=3600
  when: ansible_pkg_mgr == "apt"
  become: yes
  tags: [ base, packages, updates ]

- name: Make sure required APT packages are present (Debian)
  apt: state=present name={{ item }} force=yes
  with_items:
   - "{{ packages_install }}"
  become: yes
  ignore_errors: yes
  when: ansible_pkg_mgr == "apt"
  tags: [ base, packages, updates ]

- name: Clean APT cache
  command: apt-get clean
  args:
    warn: false
  become: yes
  when: packages_upgrade|bool and ansible_pkg_mgr == "apt"
  tags: [ base, packages, updates ]

- name: Make upgraded packages present if we are explicitly upgrading
  apt:
    upgrade: safe
    dpkg_options: 'force-confold,force-confdef'
    autoremove: yes
  become: yes
  when: packages_upgrade|bool and ansible_pkg_mgr == "apt"
  tags: [ base, packages, updates ]

- name: Ensure pip installed for system Python (RedHat)
  yum:
    name: '{{ item }}'
    state: latest
  with_items:
    - python-pip
  become: yes
  when: ansible_pkg_mgr == "yum"
  tags: [ base, config, updates ]

- name: Ensure pip installed for system Python (Debian)
  apt:
    name: '{{ item }}'
    state: latest
  with_items:
    - python-pip
  become: yes
  when: ansible_pkg_mgr == "apt"
  tags: [ base, config, updates ]

- name: Upgrade required pip components
  shell: 'pip install -U {{ item }}'
  become: yes
  with_items:
    - pip
    - setuptools
  tags: [ base, config, updates ]

- name: Create base-requirements.txt file
  template:
    src: pip/base-requirements.txt.j2
    dest: '{{ dims_etc }}/base-requirements.txt'
    owner: '{{ dims_user }}'
    group: '{{ dims_group }}'
    mode: 0o644
  become: yes
  tags: [ base, config, updates ]

- name: Ensure required system python packages present
  pip:
    requirements: '{{ dims_etc }}/base-requirements.txt'
  become: yes
  tags: [ base, config, updates ]

- name: Ensure hostname is set
  import_tasks: "{{ tasks_path }}/hostname.yml"
  tags: [ base, config ]

  # Must include tag for any task in this playbook that uses
  # apt, yum, or any site configured for squid proxying.
  # This includes tasks that are included, so pay attention to
  # the coupling of this logic. You can find them with a
  # command like this (and look for something like what is
  # shown here):
  # {% raw %}  # ... because templating in commented example is not really commented out.
  # $ egrep "tags:|apt|yum" roles/base/tasks/*
  # . . .
  # roles/base/tasks/main.yml:  apt: state=absent name={{ item }} force=yes
  # roles/base/tasks/main.yml:  tags: [ base, packages, updates ]
  # roles/base/tasks/main.yml:  apt: update_cache=yes cache_valid_time=3600
  # roles/base/tasks/main.yml:  tags: [ base, packages, updates ]
  # roles/base/tasks/main.yml:  apt: state=present name={{ item }} force=yes
  # roles/base/tasks/main.yml:  tags: [ base, packages, updates ]
  # roles/base/tasks/main.yml:  apt:
  # roles/base/tasks/main.yml:  tags: [ base, packages, updates ]
  # roles/base/tasks/main.yml:  apt:
  # roles/base/tasks/main.yml:  tags: [ base, config ]
  # . . .
  # {% endraw %}

- name: Create base /etc/hosts file (Debian, RedHat, CoreOS)
  template:
    src: '{{ item }}'
    dest: /etc/hosts
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ base_hosts }}'
        - hosts.{{ inventory_hostname }}.j2
        - hosts.category-{{ category }}.j2
        - hosts.deployment-{{ deployment }}.j2
        - hosts.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/hosts/'
        - hosts/
  become: yes
  notify: restart dnsmasq
  when: ansible_os_family in [ "Debian", "RedHat", "Container Linux by CoreOS" ] and manage_dnsmasq
  tags: [ base, hosts, config ]

- name: Disable IPv6 in kernel on non-CoreOS
  shell: "sysctl net.ipv6.conf.all.disable_ipv6=1"
  when: ansible_os_family != "Container Linux by CoreOS"
  become: yes
  tags: [ base, packages ]

- name: iptables v4 rules (Debian)
  template:
    src: '{{ item }}'
    dest: /etc/iptables/rules.v4
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
    validate: '/sbin/iptables-restore --test %s'
  with_first_found:
    - files:
        - '{{ iptables_rules }}'
        - rules.v4.{{ inventory_hostname }}.j2
        - rules.v4.category-{{ category }}.j2
        - rules.v4.deployment-{{ deployment }}.j2
        - rules.v4.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/iptables/'
        - iptables/
  notify:
    - "restart iptables ({{ ansible_distribution }}/{{ ansible_distribution_release }})"
    - "conditional restart docker"
  become: yes
  when: ansible_os_family == "Debian"
  tags: [ base, config, iptables ]

- name: iptables v4 rules (CentOS)
  template:
    src: '{{ item }}'
    dest: /etc/sysconfig/iptables
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
    validate: '/sbin/iptables-restore --test %s'
  with_first_found:
    - files:
        - '{{ iptables_rules }}'
        - rules.v4.{{ inventory_hostname }}.j2
        - rules.v4.category-{{ category }}.j2
        - rules.v4.deployment-{{ deployment }}.j2
        - rules.v4.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/iptables/'
        - iptables/
  notify:
    - "restart iptables ({{ dims_os_id }})"
    - "conditional restart docker"
  become: yes
  when: ansible_distribution == "CentOS"
  tags: [ base, config, iptables ]

- name: iptables v4 rules (CoreOS)
  template:
    src: '{{ item }}'
    dest: /var/lib/iptables/rules-save
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
    validate: '/sbin/iptables-restore --test %s'
  with_first_found:
    - files:
        - '{{ iptables_rules }}'
        - rules.v4.{{ inventory_hostname }}.j2
        - rules.v4.category-{{ category }}.j2
        - rules.v4.deployment-{{ deployment }}.j2
        - rules.v4.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/iptables/'
        - iptables/
  become: yes
  notify:
    - "restart iptables ({{ ansible_distribution }}/{{ ansible_distribution_release }})"
    - "conditional restart docker"
  when: ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, config, iptables ]

- name: iptables v6 rules (Debian)
  template:
    src: '{{ item }}'
    dest: /etc/iptables/rules.v6
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
    validate: '/sbin/ip6tables-restore --test %s'
  with_first_found:
    - files:
        - '{{ ip6tables_rules }}'
        - rules.v6.{{ inventory_hostname }}.j2
        - rules.v6.category-{{ category }}.j2
        - rules.v6.deployment-{{ deployment }}.j2
        - rules.v6.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/iptables/'
        - iptables/
  notify:
    - "restart iptables ({{ ansible_distribution }}/{{ ansible_distribution_release }})"
    - "conditional restart docker"
  become: yes
  when: ansible_os_family == "Debian"
  tags: [ base, config, iptables ]

- name: iptables v6 rules (CentOS)
  template:
    src: '{{ item }}'
    dest: /etc/sysconfig/ip6tables
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
    validate: '/sbin/ip6tables-restore --test %s'
  with_first_found:
    - files:
        - '{{ ip6tables_rules }}'
        - rules.v6.{{ inventory_hostname }}.j2
        - rules.v6.category-{{ category }}.j2
        - rules.v6.deployment-{{ deployment }}.j2
        - rules.v6.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/iptables/'
        - iptables/
  become: yes
  notify:
    - "restart iptables ({{ dims_os_id }})"
    - "conditional restart docker"
  when: ansible_distribution == "CentOS"
  tags: [ base, config, iptables ]

- name: iptables v6 rules (CoreOS)
  template:
    src: '{{ item }}'
    dest: /var/lib/ip6tables/rules-save
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
    validate: '/sbin/ip6tables-restore --test %s'
  with_first_found:
    - files:
        - '{{ ip6tables_rules }}'
        - rules.v6.{{ inventory_hostname }}.j2
        - rules.v6.category-{{ category }}.j2
        - rules.v6.deployment-{{ deployment }}.j2
        - rules.v6.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/iptables/'
        - iptables/
  become: yes
  notify:
    - "restart iptables ({{ ansible_distribution }}/{{ ansible_distribution_release }})"
    - "conditional restart docker"
  when: ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, config, iptables ]

- name: Make fail2ban.service unit present (systemd)
  template:
    src: '{{ item }}'
    dest: /etc/systemd/system/fail2ban.service
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ fail2ban_service }}'
        - 'fail2ban.service.{{ inventory_hostname }}.j2'
        - 'fail2ban.service.category-{{ category }}.j2'
        - 'fail2ban.service.deployment-{{ deployment }}.j2'
        - 'fail2ban.service.j2'
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/fail2ban.service/'
        - fail2ban.service/
  when: ansible_service_mgr == "systemd"
  become: yes
  notify:
    - "restart rsyslog"
    - "restart fail2ban ({{ ansible_service_mgr }})"
  tags: [ base, config, iptables ]

- name: Ensure /etc/fail2ban/fail2ban.local is present
  template:
    src: '{{ item }}'
    dest: /etc/fail2ban/fail2ban.local
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
  with_first_found:
    - files:
        - '{{ fail2ban_local_conf }}'
        - fail2ban.local.{{ inventory_hostname }}.j2
        - fail2ban.local.category-{{ category }}.j2
        - fail2ban.local.deployment-{{ deployment }}.j2
        - fail2ban.local.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/fail2ban/fail2ban.local/'
        - fail2ban/fail2ban.local/
  become: yes
  notify:
    - "restart rsyslog"
    - "restart fail2ban ({{ ansible_service_mgr }})"
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, iptables ]

- name: Ensure fail2ban filters are present
  file:
    src: '{{ item }}'
    dest: '/etc/fail2ban/filter.d/{{ item }}'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
  with_items: '{{ fail2ban_filters|default([]) }}'
  become: yes
  notify:
    - "restart fail2ban ({{ ansible_service_mgr }})"
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, iptables ]

- name: Ensure /etc/fail2ban/jail.local is present
  template:
    src: '{{ item }}'
    dest: '/etc/fail2ban/jail.local'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o600
  with_first_found:
    - files:
        - '{{ fail2ban_jail_local_conf }}'
        - jail.local.{{ inventory_hostname }}.j2
        - jail.local.category-{{ category }}.j2
        - jail.local.deployment-{{ deployment }}.j2
        - jail.local.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/fail2ban/jail.local/'
        - fail2ban/jail.local/
  become: yes
  notify:
    - "restart fail2ban ({{ ansible_service_mgr }})"
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, iptables ]

- name: Set timezone variables (Debian)
  copy: content={{ dims_timezone }}
        dest=/etc/timezone
        owner={{ root_user }}
        group={{ root_group }}
        mode=0o644
        backup=yes
  notify:
  - update timezone
  become: yes
  when: ansible_os_family == "Debian"
  tags: [ base, config ]

# https://coreos.com/os/docs/latest/configuring-date-and-timezone.html
- name: Set timezone (CoreOS)
  shell: timedatectl set-timezone {{ dims_timezone }}
  become: yes
  when: ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, config ]

- name: Ensure getaddrinfo configuration is present (Debian)
  template:
    src: '{{ item }}'
    dest: /etc/gai.conf
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ gai_rules }}'
        - gai.conf.{{ inventory_hostname }}.j2
        - gai.conf.category-{{ category }}.j2
        - gai.conf.deployment-{{ deployment }}.j2
        - gai.conf.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/gai/'
        - gai/
  become: yes
  when: ansible_os_family == "Debian"
  tags: [ base, config ]

# Note that /etc/environment is not read on CoreOS the same way that
# it is done on Debian based Linux systems. See the tasks/coreos.yml
# file for learning how they are applied.

- name: Populate /etc/environment (Debian, CoreOS)
  template:
    src={{ item }}
    dest=/etc/environment
    owner={{ root_user }}
    group={{ root_group }}
    mode=0o644
  with_first_found:
    - files:
        - '{{ dims_environment }}'
        - environment.{{ dims_fqdn }}.j2
        - environment.category-{{ category }}.j2
        - environment.deployment-{{ deployment }}.j2
        - environment.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/environment/'
        - environment/
  when: ansible_os_family == "Debian" or ansible_os_family == "Container Linux by CoreOS"
  become: yes
  tags: [ base, config ]

# TODO(dittrich): Do this for Darwin as well
#- name: Populate /etc/environment on Darwin
#  template: "src=environment.j2 dest=/etc/environment owner={{ root_user }} group={{ root_group }} mode=0o644"
#  when: ansible_os_family == "Darwin"
#  become: yes
#  tags: [ base, config ]

# Using "meta: flush_handlers" will run any handlers that have
# been set up to this point.
# http://docs.ansible.com/ansible/playbooks_intro.html#handlers-running-operations-on-change
- meta: flush_handlers

# TODO(mboggess): unsure if chony usable on CoreOS
#   Added "when" statement
- name: Make sure that chrony is running (Debian)
  service:
    name: chrony
    state: started
  become: yes
  when: ansible_os_family == "Debian"
  tags: [ base, services ]

- name: Check proxy availability
  import_tasks: "{{ tasks_path }}/proxy_check.yml"
  tags: [ base, config, packages, updates, triggers ]

- name: Make CoreOS resources present
  import_tasks: coreos.yml
  when: ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, packages ]

- name: Ensure DIMS system shell init hook is present (Debian, CoreOS)
  template:
    src={{ item }}
    dest=/etc/bash.bashrc
    owner={{ root_user }}
    group={{ root_group }}
    mode=0o755
  with_first_found:
    - files:
        - '{{ dims_bashrc }}'
        - bash.bashrc.{{ dims_fqdn }}.j2
        - bash.bashrc.category-{{ category }}.j2
        - bash.bashrc.deployment-{{ deployment }}.j2
        - bash.bashrc.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/bash.bashrc/'
        - bash.bashrc/
  become: yes
  when: ansible_os_family == "Debian" or ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, config ]

- name: Make DIMS system level profile present
  template:
    src={{ item }}
    dest=/etc/profile.d/dims.sh
    owner={{ root_user }}
    group={{ root_group }}
    mode=0o644
  with_first_found:
    - files:
        - '{{ dims_profile }}'
        - dims.sh.{{ dims_fqdn }}.j2
        - dims.sh.category-{{ category }}.j2
        - dims.sh.deployment-{{ deployment }}.j2
        - dims.sh.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/profile.d/'
        - profile.d/
  become: yes
  when: ansible_os_family == "Debian" or ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, config ]

- name: Make directory for DIMS bashrc plugins present
  file:
    state: directory
    path: '{{ dims_etc }}/bashrc.dims.d'
    owner: '{{ dims_user }}'
    group: '{{ dims_group }}'
    mode: 0o775
  become: yes
  tags: [ base, config ]

- name: Make DIMS-specific bashrc setup file present
  template:
    src={{ item }}
    dest={{ dims_etc }}/bashrc.dims
    owner={{ dims_user }}
    group={{ dims_group }}
    mode=0o755
  with_first_found:
    - files:
        - '{{ dims_bashrc_dims }}'
        - bashrc.dims.{{ dims_fqdn }}.j2
        - bashrc.dims.category-{{ category }}.j2
        - bashrc.dims.deployment-{{ deployment }}.j2
        - bashrc.dims.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/bashrc.dims/'
        - bashrc.dims/
  when: ansible_os_family == "Debian" or ansible_os_family == "Container Linux by CoreOS"
  become: yes
  tags: [ base, config ]

# Set up logging first to aid in debugging installation problems.

# Debian does not have a 'syslog' user/group by default, and runs rsyslogd
# as root. Since we're dropping privilege on Ubuntu, create the group and
# user here to match.

# NOTE(mboggess): CoreOS uses journald to aggregate and structure log
# output. For more information see
# https://www.loggly.com/ultimate-guide/linux-logging-with-systemd/
# Unfortunately, journald doesn't do remote logging, so we'll probably
# have to do some fenagling and maybe Docker container to run rsyslog
# on the CoreOS nodes to funnel logs back to their central holding place.
# Adding "when" statements to make these all Debian-only tasks.

- name: Stop rsyslogd
  service:
    name: rsyslog
    state: stopped
  notify: restart rsyslog
  when: ansible_os_family != "Container Linux by CoreOS"
  ignore_errors: yes
  tags: [ base, config, rsyslogd ]

- name: Add group for rsyslog
  group:
    name: syslog
    state: present
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

- name: Add non-privileged user for rsyslog
  user:
    name: syslog
    state: present
    shell: /bin/true
    group: syslog
    groups: [ 'www-data','ssl-cert' ]
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

- name: Make DIMS logging directory present
  file:
    state=directory
    path=/var/log/dims
    owner={{ rsyslog_user }}
    group={{ root_group }}
    mode=0o775
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

- name: Make /etc/rsyslog.conf present
  template:
    src: '{{ item }}'
    dest: /etc/rsyslog.conf
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ rsyslog_config }}'
        - rsyslog.conf.{{ inventory_hostname }}
        - rsyslog.conf.category-{{ category }}.j2
        - rsyslog.conf.deployment-{{ deployment }}.j2
        - rsyslog.conf.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/rsyslog/'
        - rsyslog/
      skip: true
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

- name: Ensure /etc/rsyslog.d present
  file:
    path: /etc/rsyslog.d
    state: directory
    mode: 0o644
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

- name: Make /etc/rsyslog.d/00-ignore.conf present
  template:
    src: '{{ item }}'
    dest: /etc/rsyslog.d/00-ignore.conf
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ rsyslog_ignore_config }}'
        - '00-ignore.conf.{{ inventory_hostname }}'
        - '00-ignore.conf.category-{{ category }}.j2'
        - '00-ignore.conf.deployment-{{ deployment }}.j2'
        - '00-ignore.conf.j2'
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/rsyslog.d/'
        - rsyslog.d/
      skip: true
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

# The 49-consolidation.conf file is intended to control log consolidation (i.e., "remote" log
# collection.) It effectively becomes a no-op if the host does not receive any logs
# from clients. This is being done this way to support central logging from server hosts,
# or logging to a virtual machine hypervisor host from virtual machines. (Both may actually
# be used, which is then recursive in nature.)

- name: Make /etc/rsyslog.d/49-consolidation.conf present
  template:
    src: '{{ item }}'
    dest: /etc/rsyslog.d/49-consolidation.conf
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ rsyslog_consolidation_config }}'
        - '49-consolidation.conf.{{ inventory_hostname }}'
        - '49-consolidation.conf.category-{{ category }}.j2'
        - '49-consolidation.conf.deployment-{{ deployment }}.j2'
        - '49-consolidation.conf.j2'
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/rsyslog.d/'
        - rsyslog.d/
      skip: true
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

# The 50-default.conf file is intended to be default for host configuration, with Jinja logic
# to optionally support sending to a remote log consolidation host.
- name: Make /etc/rsyslog.d/50-default.conf present
  template:
    src: '{{ item }}'
    dest: /etc/rsyslog.d/50-default.conf
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ rsyslog_default_config }}'
        - '50-default.conf.{{ inventory_hostname }}'
        - '50-default.conf.category-{{ category }}.j2'
        - '50-default.conf.deployment-{{ deployment }}.j2'
        - '50-default.conf.j2'
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/rsyslog.d/'
        - rsyslog.d/
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

- name: 'Ensure rsyslog SSL certificate directory is present'
  file:
    path: '{{ rsyslog_ssl_dir }}'
    state: directory
    owner: '{{ root_user }}'
    group: syslog
    mode: 0o750
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS" and
        rsyslog_ssl_dir is defined
  tags: [ base, config, rsyslogd ]

- name: 'Ensure directory for letsencrypt hooks is present'
  file:
    dest: /etc/letsencrypt/renewal-hooks/post
    state: directory
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o640
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS" and
        use_letsencrypt is defined and use_letsencrypt|bool
  tags: [ base, config, rsyslogd ]

- name: 'Ensure letsencrypt hook for rsyslog is present'
  template:
    src: '{{ item }}'
    dest: /etc/letsencrypt/renewal-hooks/post/rsyslog
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o640
  become: yes
  with_first_found:
    - files:
        - '{{ letsencrypt_rsyslog_hook }}'
        - renewal-hook.rsyslog.{{ inventory_hostname }}
        - renewal-hook.rsyslog.category-{{ category }}.j2
        - renewal-hook.rsyslog.deployment-{{ deployment }}.j2
        - renewal-hook.rsyslog.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/rsyslog/'
        - rsyslog/
  when: ansible_os_family != "Container Linux by CoreOS" and
        use_letsencrypt is defined and use_letsencrypt|bool
  tags: [ base, config, rsyslogd ]

- name: Configure journald for systems running systemd
  template:
    src: '{{ item }}'
    dest: /etc/systemd/journald.conf
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ journald_config }}'
        - journald.conf.{{ inventory_hostname }}
        - journald.conf.category-{{ category }}.j2
        - journald.conf.deployment-{{ deployment }}.j2
        - journald.conf.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/journald/'
        - journald/
  become: yes
  when: ansible_os_family == "Debian" and ansible_service_mgr == "systemd"
  tags: [ base, config, journald ]

# Force restart of rsyslogd here (rather than use 'notify') to
# start logging to central site as soon as possible for diagnositc
# purposes. Using 'notify' delays execution of handlers to the end.

- name: start rsyslog
  service:
    name: rsyslog
    state: started
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, rsyslogd ]

- name: /etc/logrotate.d/dims
  template:
    src: '{{ item }}'
    dest: /etc/logrotate.d/dims
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ logrotate_default_config }}'
        - dims.{{ inventory_hostname }}
        - dims.category-{{ category }}.j2
        - dims.deployment-{{ deployment }}.j2
        - dims.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/logrotate/'
        - logrotate/
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, config, logrotate ]

# Handle the configuration of network interfaces in a manner that
# matches policy for configuring bare-metal machines.
# (NetworkManager should be disabled in /etc/network/interfaces file. See
# http://support.qacafe.com/knowledge-base/how-do-i-prevent-network-manager-from-controlling-an-interface/)
#
# TODO(dittrich): Link to documentation re: bare-metal network interface config.
# TODO(mboggess): add support for other OSes as needed.

# This play is for RedHat/Fedora/CentOS
#- name: Ensure resolv.conf is not modified by changes to NICs
#  lineinfile:
#    dest=/etc/sysconfig/network
#    create=yes
#    backup=yes
#    state=present
#    line='RESOLV_MODS=no'
#    regexp=^RESOLV_MODS=
#  become: yes
#  tags: [ base, config, dns ]

- name: Make resolv.conf file present on Debian
  template:
    src: '{{ item }}'
    dest: /etc/resolv.conf
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ resolv_conf }}'
        - resolv.conf.{{ inventory_hostname }}.j2
        - resolv.conf.category-{{ category }}.j2
        - resolv.conf.deployment-{{ deployment }}.j2
        - resolv.conf.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/resolv.conf/'
        - resolv.conf/
  become: yes
  when: ansible_os_family == "Debian" and manage_resolv_conf
  tags: [ base, config, dns ]

- name: Stat /etc/NetworkManager exists
  stat:
    path: '/etc/NetworkManager'
  register: _nm_stat

- name: Make appropriate NetworkManager configruation present
  template:
    src: '{{ item }}'
    dest: /etc/NetworkManager/NetworkManager.conf
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ network_manager }}'
        - NetworkManager.conf.{{ inventory_hostname }}.j2
        - NetworkManager.conf.{{ ansible_lsb.codename }}.j2
        - NetworkManager.conf.category-{{ category }}.j2
        - NetworkManager.conf.deployment-{{ deployment }}.j2
        - NetworkManager.conf.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/NetworkManager/'
        - NetworkManager/
  become: yes
  when: ansible_os_family == "Debian" and
        _nm_stat is defined and
        _nm_stat.stat.exists
  ignore_errors: true
  tags: [ base, config ]

# Deal with dnsmasq configuration in the face of NetworkManager
# and/or Ubuntu 14.04 issues.
# TODO(mboggess): Tasks in dnsmasq.yml fail on CoreOS
# added "when" statement
- name: Make DNS service using dnsmasq present (Debian)
  import_tasks: dnsmasq.yml
  when: ansible_os_family == "Debian" and manage_dnsmasq
  tags: [ base, packages ]

# Handle the configuration of network interfaces in a manner that
# matches policy for configuring bare-metal machines.
# (NetworkManager should be disabled in /etc/network/interfaces file. See
# http://support.qacafe.com/knowledge-base/how-do-i-prevent-network-manager-from-controlling-an-interface/)
#
# TODO(dittrich): Link to documentation re: bare-metal network interface config.
# TODO(mboggess): add support for other OSes as needed.
- name: Apply /etc/network/interfaces (Debian)
  template:
    src: '{{ item }}'
    dest: /etc/network/interfaces
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ network_interfaces }}'
        - interfaces.{{ inventory_hostname }}.j2
        - interfaces.category-{{ category }}.j2
        - interfaces.deployment-{{ deployment }}.j2
        - interfaces.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/network/'
        - network/
  become: yes
  ignore_errors: true
  when: ansible_os_family == "Debian" and manage_interfaces
  tags: [ base, config, interfaces ]

# http://www.tecmint.com/configure-network-connections-using-nmcli-tool-in-linux/
# nmcli con add type ethernet con-name static2 ifname eth1 ip4 192.168.29.13/24 gw4 192.168.29.1

#- name: Install needed network-manager libs
#  apt:
#    name: '{{ item }}'
#    state: installed
#  with_items:
#    - libnm-glib-dev
#  become: yes
#  when: ansible_pkg_mgr == "apt"
#  tags: [ base, config ]

#     - libnm-qt-devel.x86_64
#     - nm-connection-editor.x86_64
#     - libsemanage-python
#     - policycoreutils-python

# Roadblock.
# Fails with: Error: invalid property 'address': 'address' is ambiguous (addresses x address-labels).
#- name: Create ethernet device config using nmcli
#  nmcli:
#    conn_name={{ networking.interfaces[1].device }}
#    ifname={{ networking.interfaces[1].device }}
#    type=ethernet
#    state=present
#    autoconnect=yes
#    ip4={{ networking.interfaces[1].ip }}/{{ networking.interfaces[1].cidr_bits }}
#    gw4={{ networking.interfaces[1].gateway }}
#    dns4={{ networking.interfaces[1].dns_servers }}
#  become: yes
#  when: networking is defined and inventory_hostname == "yellow.devops.local"

# As Bill O'Reilly would say, "#%$^$! We'll do it live!"
#- name: Create ethernet device config using nmcli
#  shell: >
#    /usr/bin/nmcli connection add type ethernet
#    con-name {{ networking.interfaces[1].device }}
#    ifname {{ networking.interfaces[1].device }}
#    ip4 {{ networking.interfaces[1].ip }}/{{ networking.interfaces[1].cidr_bits }}
#    gw4 {{ networking.interfaces[1].gateway }}
#  become: yes
#  when: 'networking is defined and inventory_hostname in ["yellow.devops.local", "yellow.ops.ectf"]'
#  when: ansible_distribution == "Debian" and ansible_distribution_major_version|int < 8
#  tags: [ base, config ]

- name: Ensure functioning /etc/fstab file is present
  template:
    src: '{{ item }}'
    dest: /etc/fstab
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ base_fstab }}'
        - fstab.{{ inventory_hostname }}.j2
        - fstab.category-{{ category }}.j2
        - fstab.deployment-{{ deployment }}.j2
        - fstab.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/fstab/'
        - fstab/
      skip: true
  become: yes
  when: ansible_os_family == "Debian" or ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, config ]

# The Google shflags package is used by DIMS scripts
- name: Make sure shflags is present
  import_tasks: "{{ tasks_path }}/install-shflags.yml"
  tags: [ base, updates ]

# Note: All scripts end up without extensions. In the directory, they
# have names that end in .j2 or .sh to differentiate between templated
# and non-templated scripts.

- name: Ensure Makefile.dims.global is present
  template:
    src: '{{ files }}/common/Makefile.dims.global'
    dest: '{{ dims_etc }}/Makefile.dims.global'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o755
  become: yes
  tags: [ base, packages, scripts, tests ]

- name: Make sure common (templated) scripts are present
  template:
    src: '{{ item }}'
    dest: '{{ dims_bin }}/{{ item | basename | regex_replace("\.j2$","") }}'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o755
  with_fileglob:
   - '{{ files }}/common-scripts/*.j2'
  become: yes
  tags: [ base, packages, scripts, tests ]

# These are used by tests, so the 'tests' tag is applied specifically for this library.
- name: Make DIMS bash shell functions present
  copy:
    src=dims_functions.sh
    dest={{ dims_bin }}/dims_functions.sh
    owner={{ dims_user }}
    group={{ dims_group }}
    mode=0o644
  become: yes
  tags: [ base, config, tests ]

- name: Make sure common (non-templated) BASH scripts are present
  copy:
    src: '{{ item }}'
    dest: '{{ dims_bin }}/{{ item | basename | regex_replace("\.sh$","") }}'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o755
  with_fileglob:
   - '{{ files }}/common-scripts/*.sh'
  become: yes
  tags: [ base, packages, scripts, tests ]

- name: Make sure common (non-templated) Python scripts are present
  copy:
    src: '{{ item }}'
    dest: '{{ dims_bin }}/{{ item | basename | regex_replace("\.py$","") }}'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o755
  with_fileglob:
   - '{{ files }}/common-scripts/*.py'
  become: yes
  tags: [ base, packages, scripts, tests ]

- name: Ensure Ubuntu locale set (Ubuntu)
  shell: "locale-gen {{ dims_locale }}"
  notify:
  - update Ubuntu locales
  become: yes
  when: ansible_distribution == "Ubuntu"
  tags: [ base, packages ]

- name: Ensure Debian locale set (Debian)
  lineinfile: dest=/etc/locale.gen
              regexp='^# {{ dims_locale }} UTF-8'
              line='{{ dims_locale }} UTF-8'
              backrefs=yes
              state=present
  notify:
  - update Debian locales
  become: yes
  when: ansible_distribution == "Debian"
  tags: [ base, packages ]

# NOTE(mboggess): on CoreOS, /etc/ssh/sshd_config is a symlink to /usr/share/ssh/sshd_config
# and /usr is a readonly filesystem. Might need to use user-data file for this step...
- name: Apply SSH daemon configuration (OS invariant)
  template:
    src: '{{ item }}'
    dest: /etc/ssh/sshd_config
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ sshd_config }}'
        - sshd_config.{{ inventory_hostname }}.j2
        - sshd_config.category-{{ category }}.j2
        - sshd_config.deployment-{{ deployment }}.j2
        - sshd_config.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/sshd/'
        - sshd/
  become: yes
  when: ansible_os_family == "Debian"
  notify:
  - restart ssh
  tags: [ base, config ]

- name: Customize /etc/rc.local (Debian)
  template:
    src: '{{ item }}'
    dest: /etc/rc.local
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o550
  with_first_found:
    - files:
        - '{{ base_rclocal }}'
        - rc.local.{{ inventory_hostname }}.j2
        - rc.local.category-{{ category }}.j2
        - rc.local.deployment-{{ deployment }}.j2
        - rc.local.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/rc.local/'
        - rc.local/
  become: yes
  when: ansible_os_family == "Debian"
  tags: [ base, config ]

- name: Ensure bashrc additions are present
  template:
    src: '{{ item }}'
    dest: '{{ dims_etc_bashrc }}/{{ item | basename | regex_replace("\.j2","") }}'
    owner: '{{ dims_user }}'
    group: '{{ dims_group }}'
    mode: 0o644
  with_fileglob:
   - ../templates/bashrc.d/*.bash.j2
  become: yes
  tags: [ base, config ]

- name: Ensure bashpass is present
  import_tasks: "{{ tasks_path }}/install-bashpass.yml"
  become: yes
  when: ansible_os_family != "Container Linux by CoreOS"
  tags: [ base, packages, updates ]

# See Jira Ticket http://jira.prisem.washington.edu/browse/DIMS-431
#
# The following two plays can be handled in all instances by now inserting
# the inclusion logic into the postactivate script in the Python virtual
# environment $WORKON_HOME directory. That file is run after every Python
# virtual environment is run. The virtual environment includes scripts for
# DIMS development that use hubflow and Git, so this is an acceptable place
# to run them.

# TODO(dittrich): How to handle this for Mac?
- name: Install hub bash profile settings
  copy:
    src: hub.bash_completion.sh
    dest: '{{ dims_etc_bashrc }}/hub.bash_completion.sh'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o755
  become: yes
  when: ansible_os_family == "Debian" or ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, config ]

# TODO(dittrich): How to handle this for Mac?
- name: Install git prompt script
  copy:
    src: git-prompt.sh
    dest: '{{ dims_etc_bashrc }}/git-prompt.sh'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o755
  become: yes
  when: ansible_os_family == "Debian" or ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, config ]

- name: Ensure bats is present
  import_tasks: "{{ tasks_path }}/install-bats.yml"
  become: yes
  tags: [ base, packages, updates ]

- name: Make defined bats tests present
  import_tasks: "{{ tasks_path }}/install-bats-tests.yml"
  become: yes
  tags: [ base, tests ]

- name: Make defined triggers present
  import_tasks: "{{ tasks_path }}/triggers.yml"
  become: yes
  tags: [ base, triggers ]

- name: Make CoreOS resources present
  import_tasks: coreos.yml
  become: yes
  when: ansible_os_family == "Container Linux by CoreOS"
  tags: [ base, packages ]

- name: Set recipient for root/posmaster emails
  template:
    src: '{{ item }}'
    dest: /etc/aliases
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  with_first_found:
    - files:
        - '{{ postfix_aliases }}'
        - aliases.{{ inventory_hostname }}.j2
        - aliases.{{ host_group }}.j2
        - aliases.{{ postfix_group }}.j2
        - aliases.deployment-{{ deployment }}.j2
        - aliases.j2
      paths:
        - '{{ dims_private }}/roles/{{ role_name }}/templates/aliases/'
        - aliases/
  when: not ( ( groups.postfix is defined and inventory_hostname in groups.postfix )
           or ( groups.trident is defined and inventory_hostname in groups.trident ) )
  register: _aliases
  tags: [ base, config ]

  # Running "newaliases" fails on CentOS when Postfix main.cf contains
  # "inet_protocols = all" with the following error message:
  # newaliases: fatal: parameter inet_interfaces: no local interface found for ::1

- name: Ensure inet_protocols set properly on CentOS
  lineinfile:
    dest: /etc/postfix/main.cf
    regexp: '^inet_protocols'
    line: 'inet_protocols = ipv4'
    owner: '{{ root_user }}'
    group: '{{ root_group }}'
    mode: 0o644
  when: ansible_distribution == "CentOS" and
        not '::1' in ansible_all_ipv6_addresses
  tags: [ base, config ]

- name: Reload aliases
  shell: newaliases
  become: yes
  when: _aliases.changed
  tags: [ base, config ]

- name: Restart postfix
  service:
    name: postfix
    state: restarted
  become: yes
  when: _aliases.changed
  tags: [ base, config ]

# Using "meta: flush_handlers" will run any handlers that have
# been set up to this point.
# http://docs.ansible.com/ansible/playbooks_intro.html#handlers-running-operations-on-change
- meta: flush_handlers

- name: Ensure cron environment is present
  cronvar:
    cron_file: 'dims'
    name: '{{ item.name }}'
    value: '{{ item.value }}'
  with_items:
    - "{{ cronvars }}"
  become: yes
  when: cronvars is defined
  tags: [ base, config ]

- name: Ensure DIMS cron jobs are present
  cron:
    cron_file: 'dims'
    name: '{{ item.name|default("") }}'
    weekday: '{{ item.weekday|default("*") }}'
    hour: '{{ item.hour|default("0") }}'
    minute: '{{ item.minute|default("0") }}'
    user: '{{ item.user|default("ansible") }}'
    job: '{{ item.job }}'
  with_items:
    - "{{ cronjobs }}"
  become: yes
  when: cronjobs is defined
  tags: [ base, config ]

  # This is not DRY, but base role does not use post_tasks.yml
- name: Log end of 'base' role
  shell: >
    [ -f {{ dims_bin }}/dims.logger ] &&
    {{ dims_bin }}/dims.logger
    --logger-tag ansible-playbook
    --logmon-exchange devops
    'Ending role: base'
  ignore_errors: yes
  tags: [ base, config, dns, logrotate, packages, rsyslogd, tests, triggers ]

# vim: ft=ansible :
